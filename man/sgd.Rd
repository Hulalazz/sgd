% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/sgd.R
\name{sgd}
\alias{sgd}
\alias{sgd.big.matrix}
\alias{sgd.formula}
\alias{sgd.function}
\alias{sgd.matrix}
\title{Stochastic gradient descent}
\usage{
sgd(x, ...)

\method{sgd}{formula}(formula, data, model, model.control = list(),
  sgd.control = list(...), ...)

\method{sgd}{function}(x, fn.control = list(), sgd.control = list(...), ...)

\method{sgd}{matrix}(x, y, model, model.control = list(),
  sgd.control = list(...), ...)

\method{sgd}{big.matrix}(x, y, model, model.control = list(),
  sgd.control = list(...), ...)
}
\arguments{
\item{x}{for \code{sgd.function}, x is a function to minimize; for
\code{sgd.matrix}, x is a design matrix.}

\item{formula}{an object of class \code{"\link{formula}"} (or one that can be
coerced to that class): a symbolic description of the model to be fitted.
The details can be found in \code{"\link{glm}"}.}

\item{data}{an optional data frame, list or environment (or object coercible
by \code{\link[base]{as.data.frame}} to a data frame) containing the
variables in the model. If not found in data, the variables are taken from
environment(formula), typically the environment from which glm is called.}

\item{model}{character specifying the model to be used: \code{"lm"} (linear
model), \code{"glm"} (generalized linear model), \code{"ee"} (estimating
equation).}

\item{model.control}{a list of parameters for controlling the model.
\itemize{
  \item family (\code{"glm"}): a description of the error distribution and
    link function to be used in the model. This can be a character string
    naming a family function, a family function or the result of a call to
    a family function.  (See \code{\link[stats]{family}} for details of
    family functions.)
  \item rank logical. Should the rank of the design matrix be checked?
  \item fn (\code{"ee"}): function \eqn{g(\theta,x)} which returns a
    \eqn{k}-vector corresponding to the \eqn{k} moment conditions. It is a
    required argument if \code{gr} not specified
  \item gr (\code{"ee"}): gradient of the moment function, which if not
    passed in defaults to taking the numerical gradient of \code{fn}
  \item type (\code{"ee"}): character specifying the generalized method of
    moments procedure: \code{"twostep"} (Hansen, 1982), \code{"iterative"}
    (Hansen et al., 1996). Defaults to \code{"iterative"}.
  \item wmatrix (\code{"ee"}): weighting matrix to be used in the loss
    function. Defaults to the identity matrix.
}}

\item{sgd.control}{a list of parameters for controlling the estimation
\itemize{
  \item method: character specifying the method to be used: \code{"sgd"},
  \code{"implicit"}, \code{"asgd"}, \code{"ai-sgd"}. Default is
  \code{"implicit"}.  See \sQuote{Details}.
  \item lr: character specifying the learning rate to be used:
    \code{"one-dim"}, \code{"one-dim-eigen"}, \code{"d-dim"},
    \code{"adagrad"}, \code{"rmsprop"}. Default is \code{"one-dim"}.
    See \sQuote{Details}.
  \item start: starting values for the parameter estimates. Default is
    random initialization around the mean.
  \item weights: an optional vector of "prior weights" to be used in the
    fitting process. Should be NULL or a numeric vector.
  \item offset: this can be used to specify an a priori known component to
    be included in the linear predictor during fitting. This should be NULL
    or a numeric vector of length equal to the number of cases. One or more
    offset terms can be included in the formula instead or as well, and if
    more than one is specified their sum is used. See
    \code{\link[stats]{offset}}
  \item npasses: the number of passes for sgd
  \item lr.control: scalar hyperparameter one can tweak dependent on the
    learning rate
}}

\item{fn.control}{for \code{sgd.function}, it is a list of controls for the
function.}

\item{y}{for {sgd.matrix}, y is a vector of observations, with length equal
to the number of rows in x.}

\item{\dots}{arguments to be used to form the default \code{sgd.control}
arguments if it is not supplied directly.}
}
\value{
An object of class \code{"sgd"}, which is a list containing at least the
following components:

\code{coefficients}
a named vector of coefficients

\code{converged}
logical. Was the algorithm judged to have converged?

\code{model.out}
a list of model-specific output attributes

\code{estimates}
TODO

\code{times}
vector of times in seconds it took to complete the number of iterations to
achieve the corresponding estimate
}
\description{
Run stochastic gradient descent on the underlying loss function for a given
model and data, or a user-specified loss function.
}
\details{
Methods:
\itemize{
  \item \code{sgd}: stochastic gradient descent (Robbins and Monro, 1951)
  \item \code{implicit}: implicit stochastic gradient descent (Toulis et al.,
    2014)
  \item \code{asgd} stochastic gradient with averaging (Polyak and Juditsky,
    1992)
  \item \code{ai-sgd} implicit stochastic gradient with averaging (Toulis et
    al., 2015)
}

Learning rates:
\itemize{
  \item \code{one-dim}: scalar value prescribed in Xu (2011) as
    \code{a_n = scale * gamma/(1 + alpha*gamma*n)^(-c)}
    where the defaults are \code{scale=1}, \code{gamma=1}, \code{alpha} the
    minimum eigenvalue of the covariance of the data's design matrix, and
    \code{c} is \code{1} if implemented without averaging, \code{2/3} if with
    averaging
  \item \code{one-dim-eigen}: diagonal matrix
    \code{}
  \item \code{d-dim}: diagonal matrix
    \code{}
  \item \code{adagrad}: diagonal matrix prescribed in Duchi et al. (2011) as
    \code{}
  \item \code{rmsprop}: diagonal matrix prescribed in Tieleman and Hinton
    (2012) as
    \code{}
}
}
\examples{
## Dobson (1990, p.93): Randomized Controlled Trial
counts <- c(18, 17, 15, 20, 10, 20, 25, 13, 12)
outcome <- gl(3, 1, 9)
treatment <- gl(3, 3)
print(d.AD <- data.frame(treatment, outcome, counts))
sgd.D93 <- sgd(counts ~ outcome + treatment, model="glm",
               model.control=list(family = poisson()))
sgd.D93

## Venables & Ripley (2002, p.189): an example with offsets
utils::data(anorexia, package="MASS")

anorex.1 <- sgd(Postwt ~ Prewt + Treat + offset(Prewt),
                data=anorexia, model="lm")
}
\author{
Dustin Tran, Tian Lan, Panos Toulis, Ye Kuang, Edoardo Airoldi
}
\references{
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for
online learning and stochastic optimization. \emph{Journal of Machine
Learning Research}, 12:2121-2159, 2011.

Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic
approximation by averaging. \emph{SIAM Journal on Control and Optimization},
30(4):838-855, 1992.

Herbert Robbins and Sutton Monro. A stochastic approximation method.
\emph{The Annals of Mathematical Statistics}, pp. 400-407, 1951.

Panos Toulis, Jason Rennie, and Edoardo M. Airoldi, "Statistical analysis of
stochastic gradient methods for generalized linear models", In
\emph{Proceedings of the 31st International Conference on Machine Learning},
2014.

Panos Toulis, Dustin Tran, and Edoardo M. Airoldi, "Stability and optimality
in stochastic gradient descent", arXiv preprint arXiv:1505.02417, 2015.

Wei Xu. Towards optimal one pass large scale learning with averaged
stochastic gradient descent. arXiv preprint arXiv:1107.2490, 2011.
}

